{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5100920d-3c64-422e-9586-7c7ed044ed7d",
   "metadata": {},
   "source": [
    "# Maximum marginal relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35107f50-34ae-46c7-aa61-638d3bac5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7fb3f0-302a-45ec-bd9d-908487db6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_similarity(query_vector, document_vector):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between the query vector and a document vector.\n",
    "\n",
    "    Args:\n",
    "    - query_vector (np.array): Vector representing the query.\n",
    "    - document_vector (np.array): Vector representing a document.\n",
    "\n",
    "    Returns:\n",
    "    - similarity (float): Cosine similarity between the query vector and the document vector.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(query_vector, document_vector)\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    doc_norm = np.linalg.norm(document_vector)\n",
    "    similarity = dot_product / (query_norm * doc_norm)\n",
    "    return similarity\n",
    "\n",
    "def mmr_reranking(documents, query_vector, alpha, beta, initial_ranking):\n",
    "    \"\"\"\n",
    "    Rerank a list of documents using Maximum Marginal Relevance (MMR).\n",
    "\n",
    "    Args:\n",
    "    - documents (list): List of document vectors.\n",
    "    - query_vector (np.array): Vector representing the query.\n",
    "    - alpha (float): Weight parameter for relevance.\n",
    "    - beta (float): Weight parameter for diversity.\n",
    "    - initial_ranking (list): Initial ranked list of document indices.\n",
    "\n",
    "    Returns:\n",
    "    - reranked_indices (list): Reranked list of document indices.\n",
    "    \"\"\"\n",
    "    num_documents = len(documents)\n",
    "    reranked_indices = []\n",
    "\n",
    "    for index in initial_ranking:\n",
    "        remaining_indices = [i for i in initial_ranking if i not in reranked_indices]\n",
    "        remaining_documents = [documents[i] for i in remaining_indices]\n",
    "\n",
    "        # Calculate relevance score\n",
    "        relevance_score = calculate_similarity(query_vector, documents[index])\n",
    "\n",
    "        # Calculate diversity score\n",
    "        diversity_scores = [calculate_similarity(documents[index], doc) for doc in remaining_documents]\n",
    "        max_diversity_score = max(diversity_scores)\n",
    "        diversity_score = max_diversity_score if len(diversity_scores) > 1 else 0  # Set diversity_score to 0 if only 1 document left\n",
    "\n",
    "        # Calculate MMR score\n",
    "        mmr_score = alpha * relevance_score - beta * diversity_score\n",
    "\n",
    "        # Select document with maximum MMR score\n",
    "        selected_index = remaining_indices[np.argmax(mmr_score)]\n",
    "        reranked_indices.append(selected_index)\n",
    "\n",
    "    return reranked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6471c7a-fd73-4958-95a5-91bc77d4658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked Documents: [[0.7, 0.8, 0.9], [0.5, 0.6, 0.7], [0.4, 0.5, 0.6], [0.1, 0.2, 0.3], [0.2, 0.3, 0.4]]\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have 5 documents, each represented by a 3-dimensional vector\n",
    "documents = [\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6],\n",
    "    [0.7, 0.8, 0.9],\n",
    "    [0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7]\n",
    "]\n",
    "query_vector = np.array([0.1, 0.2, 0.3])\n",
    "alpha = 0.7  # Weight parameter for relevance\n",
    "beta = 0.3   # Weight parameter for diversity\n",
    "initial_ranking = [2, 4, 1, 0, 3]  # Initial ranked list of document indices\n",
    "\n",
    "reranked_indices = mmr_reranking(documents, query_vector, alpha, beta, initial_ranking)\n",
    "reranked_documents = [documents[i] for i in reranked_indices]\n",
    "print(\"Reranked Documents:\", reranked_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110752f1-d60b-41b8-8e19-742fe0649a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ec403a-048c-41cc-9cfa-752d97f98303",
   "metadata": {},
   "source": [
    "## MMR with LangChain\n",
    "\n",
    "Source: https://github.com/generative-ai-on-aws/generative-ai-on-aws/blob/main/09_rag/01_langchain_llama2_sagemaker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab370b-d856-481b-9516-3990054e0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.309 faiss-cpu==1.7.4 pypdf==3.15.1 -q --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b91caa-2436-4232-a857-0bc8982f3bba",
   "metadata": {},
   "source": [
    "### Fetch sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c0e13-57d9-48f1-b663-eaa2063c7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2022, source=filenames[0]),\n",
    "    dict(year=2021, source=filenames[1]),\n",
    "    dict(year=2020, source=filenames[2]),\n",
    "    dict(year=2019, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3007f80-1f20-4725-b725-2fb16baa3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "for local_pdf in local_pdfs:\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    for pagenum in range(len(pdf_reader.pages)-3):\n",
    "        page = pdf_reader.pages[pagenum]\n",
    "        pdf_writer.add_page(page)\n",
    "\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f09d33-50b5-4511-af41-184d7a3446c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "        \n",
    "    documents += document\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'# of Document Pages {len(documents)}')\n",
    "print(f'# of Document Chunks: {len(docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06cff0-3a91-4768-ae02-e63f2140aaae",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8baf01-b1cf-4ada-a770-899d6d57ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "embedding_model_id, embedding_model_version = \"huggingface-textembedding-all-MiniLM-L6-v2\", \"*\"\n",
    "model = JumpStartModel(model_id=embedding_model_id, model_version=embedding_model_version)\n",
    "embedding_predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb9162-bfc8-4a35-930a-0fcf155d7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_endpoint_name = embedding_predictor.endpoint_name\n",
    "embedding_model_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a87d43-026b-4b33-8969-885db3dc46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "aws_region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d586ac-6466-40ee-978b-db185c215f1a",
   "metadata": {},
   "source": [
    "### Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed183f25-2ced-48a1-9532-7d0699c8e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "import json\n",
    "\n",
    "\n",
    "class CustomEmbeddingsContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]\n",
    "\n",
    "\n",
    "embeddings_content_handler = CustomEmbeddingsContentHandler()\n",
    "\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_model_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    content_handler=embeddings_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad7600-0e4c-4f52-a16a-f11967d1f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad0734-587a-4075-adef-45be8849d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f57463-d580-4a9a-811e-283c70a14c40",
   "metadata": {},
   "source": [
    "### Creating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a1efe-a412-4cb5-be30-80012d03b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Use the context provided to answer the question at the end. If you dont know the answer just say that you don't know, don't try to make up an answer.\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a035391-1dbb-4b05-bb56-5a7bb3579613",
   "metadata": {},
   "source": [
    "### Preparing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcce9c6-c6b3-4cf5-a790-afb84b49f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "\n",
    "\n",
    "class QAContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            {\"inputs\" : [\n",
    "                [\n",
    "                    {\n",
    "                        \"role\" : \"system\",\n",
    "                        \"content\" : \"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\" : \"user\",\n",
    "                        \"content\" : prompt\n",
    "                    }\n",
    "                ]],\n",
    "                \"parameters\" : {**model_kwargs}\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generation\"][\"content\"]\n",
    "\n",
    "qa_content_handler = QAContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18a899-3e63-4b51-bb72-14384f524330",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id, llm_model_version = \"meta-textgeneration-llama-2-7b-f\", \"2.*\"\n",
    "llm_model = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version)\n",
    "llm_predictor = llm_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c2668-84a4-44b8-a371-8ac7274d360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_endpoint_name = llm_predictor.endpoint_name\n",
    "llm_model_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc34d5-ab9a-4251-9f17-b5dbc3c338b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SagemakerEndpoint(\n",
    "        endpoint_name=llm_model_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"max_new_tokens\": 1000, \"top_p\": 0.9, \"temperature\": 1e-11},\n",
    "        endpoint_kwargs={\"CustomAttributes\": 'accept_eula=true'},\n",
    "        content_handler=qa_content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393e5e7-e172-47ea-9394-4504531e04af",
   "metadata": {},
   "source": [
    "### Retrieval with MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8e952-f9cd-48c4-b927-8642b17fc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(\n",
    "        search_type=\"mmr\", # Maximum Marginal Relevance (MMR)\n",
    "        search_kwargs={\"k\": 3, \"lambda_mult\": 0.1}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505310b6-7159-41c9-a973-d27768112bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How has AWS evolved?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(f'Query: {result[\"query\"]}\\n')\n",
    "print(f'Result: {result[\"result\"]}\\n')\n",
    "print(f'Context Documents: ')\n",
    "for srcdoc in result[\"source_documents\"]:\n",
    "      print(f'{srcdoc}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
