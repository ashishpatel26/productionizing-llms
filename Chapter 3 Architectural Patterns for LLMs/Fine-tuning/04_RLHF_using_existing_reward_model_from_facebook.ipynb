{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fd6f4-0a04-4a80-bf3c-4d295c8dfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --disable-pip-version-check -q \\\n",
    "    torch==2.0.1 \\\n",
    "    transformers==4.34.1 \\\n",
    "    datasets==2.12.0 \\\n",
    "    accelerate==0.23.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    trl==0.7.2 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    typing_extensions==4.7.1 \\\n",
    "    bitsandbytes==0.41.1 \\\n",
    "    peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e69fbd-e7ae-49b3-bdff-e53632f03a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r peft_ranking_reward_public_qanda_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a05c5f-9971-4811-85bc-d9bc7f4351a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peft_ranking_reward_public_qanda_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b7e44-c783-4015-a2b3-5024634448b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e05847-2b41-48fe-a439-af7df43b25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_fine_tuned_with_ranking_rewards_llama2_checkpoint = './peft_fine_tuned_with_ranking_rewards_llama2'\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n",
    "    tokenizer_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-hf\", metadata={\"help\": \"the tokenizer name\"})\n",
    "    reward_model_name: Optional[str] = field(default=peft_ranking_reward_public_qanda_checkpoint, metadata={\"help\": \"the reward model name\"})\n",
    "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n",
    "    output_max_length: Optional[int] = field(default=128, metadata={\"help\": \"maximum length for generation\"})\n",
    "    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n",
    "    batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the batch size\"})\n",
    "    ppo_epochs: Optional[int] = field(default=4, metadata={\"help\": \"the number of ppo epochs\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    adafactor: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the adafactor optimizer\"})\n",
    "    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"whether to early stop\"})\n",
    "    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"kl target for early stopping\"})\n",
    "    reward_baseline: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"a baseline value that is subtracted from the reward\"},\n",
    "    )\n",
    "    batched_gen: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the batched text gen\"})\n",
    "    save_freq: Optional[int] = field(default=None, metadata={\"help\": \"n steps to save the model\"})\n",
    "#    output_dir: Optional[str] = field(default=\"runs/\", metadata={\"help\": \"n steps to save the model\"})\n",
    "    seed: Optional[int] = field(default=0, metadata={\"help\": \"the seed\"})\n",
    "    steps: Optional[int] = field(default=100, metadata={\"help\": \"number of epochs\"})\n",
    "    init_kl_coef: Optional[float] = field(\n",
    "        default=0.2,\n",
    "        metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n",
    "    )\n",
    "\n",
    "    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4615f99-f337-4ac1-9b94-dfe9363b2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args: ScriptArguments = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
    "\n",
    "dataset_name = \"lvwerra/stack-exchange-paired\"\n",
    "config = PPOConfig(\n",
    "    steps=script_args.steps,\n",
    "    model_name=script_args.model_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    batch_size=script_args.batch_size,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=script_args.early_stopping,\n",
    "    target_kl=script_args.target_kl,\n",
    "    ppo_epochs=script_args.ppo_epochs,\n",
    "    seed=script_args.seed,\n",
    "    init_kl_coef=script_args.init_kl_coef,\n",
    "    adap_kl_ctrl=script_args.adap_kl_ctrl,\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)\n",
    "\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efca0a7-8a66-46c8-90bf-109ab861d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(\n",
    "    tokenizer,\n",
    "    dataset_name=\"lvwerra/stack-exchange-paired\",\n",
    "    data_dir=\"data/rl\",\n",
    "    split=\"train\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(dataset_name, data_dir=data_dir, split=split)\n",
    "    original_columns = ds.column_names\n",
    "    num_proc = 24\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        new_examples = {\n",
    "            \"query\": [],\n",
    "            \"input_ids\": [],\n",
    "        }\n",
    "        for question in examples[\"question\"]:\n",
    "            query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            tokenized_question = tokenizer(query, truncation=True)\n",
    "            new_examples[\"query\"].append(query)\n",
    "            new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "\n",
    "        return new_examples\n",
    "\n",
    "    ds = ds.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )\n",
    "    ds = ds.filter(lambda x: len(x[\"input_ids\"]) < 512, batched=False)\n",
    "\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc495b4-5b39-4ee0-b5a7-a93caa7f28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff579a7-b8d2-4ef0-b066-0dacfde78d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "train_dataset = build_dataset(tokenizer, \"lvwerra/stack-exchange-paired\", data_dir=\"data/rl\", split=\"train\")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c9882-c21e-4fe5-b79e-73bb974541a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "current_device = Accelerator().local_process_index\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427faba6-91ea-420b-ab00-1450ffaddbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import create_reference_model\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "ref_model = create_reference_model(model)\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50229e3-f5fa-40eb-897e-b6cd93202a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "optimizer = None\n",
    "if script_args.adafactor:\n",
    "    optimizer = Adafactor(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        scale_parameter=False,\n",
    "        relative_step=False,\n",
    "        warmup_init=False,\n",
    "        lr=config.learning_rate,\n",
    "    )\n",
    "    \n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# We then build the sentiment analysis pipeline using our reward model, passing the\n",
    "# model name and the sentiment analysis pipeline arguments. Let's also make sure to\n",
    "# set the device to the same device as the PPOTrainer.\n",
    "\n",
    "reward_model_tokenizer = AutoTokenizer.from_pretrained(script_args.reward_model_name)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a ` pipeline` bug\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=script_args.reward_model_name,\n",
    "    device_map={\"\": current_device},\n",
    "    model_kwargs={\"load_in_8bit\": True},\n",
    "    tokenizer=reward_model_tokenizer,\n",
    "    return_token_type_ids=False,\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    # \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000,\n",
    "}\n",
    "output_min_length = 32\n",
    "output_max_length = script_args.output_max_length\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08738b4-605b-415d-8fd0-6feb122d15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if epoch >= config.total_ppo_epochs:\n",
    "        break\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        length_sampler=output_length_sampler,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "    # Compute reward score (using the sentiment analysis pipeline)\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n",
    "        #ppo_trainer.save_pretrained(script_args.output_dir + f\"step_{epoch}\")\n",
    "        ppo_trainer.tokenizer.save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint)\n",
    "        ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint) # merge\n",
    "        #ppo_trainer.model.save_pretrained(peft_fine_tuned_with_detoxification_rewards_checkpoint)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c269c7-4697-459d-bfae-aa139f26b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.tokenizer.save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint)\n",
    "ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint) # merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3d181-2e3f-4520-a28c-aaf3f227f5cf",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d43e2-c5e0-4642-88d4-67d0b094aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, AutoPeftModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184b848-d5ea-4411-91f6-35980245231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_fine_tuned_with_ranking_rewards_llama2_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ded890-3bcc-4a40-aa42-6c7872509673",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ce3c4-1873-4464-887e-1f4abe1b9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    toxicity_model_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    toxicity_model_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35fd36-ebbb-4029-a041-bb876b21832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_toxic_text = \"You are a great person and i like you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f1755-375c-408e-802e-d12764cf1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_text = \"You are a terrible person and i hate you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist() \n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba5fd9-12a2-491b-b9d3-b18a6077e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                          model=toxicity_model_name, \n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output for non-toxic text:\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"\\nReward model output for toxic text:\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e0ada-ab6c-42fa-aefa-bcaf44999046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88930d-04d8-41bd-ae7c-13fd5c5b6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743252d-788c-449a-8882-701cfe0da33a",
   "metadata": {},
   "source": [
    "## Evaluate toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7b068-12c8-4204-9075-6ee4d10d540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\", \n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2498d3d-5c1f-410b-80e3-f4e62adbcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d7b04-dddd-4905-811b-02596db4d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_dataset(tokenizer, \"lvwerra/stack-exchange-paired\", data_dir=\"data/evaluation\", split=\"train\")\n",
    "test_dataset = test_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b00656-ab23-413c-a886-316d1768f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_toxicity(model, \n",
    "                      toxicity_evaluator, \n",
    "                      tokenizer, \n",
    "                      dataset, \n",
    "                      num_samples):\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "            \n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "        \n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean & std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "        \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51307991-e1e4-494d-a1f4-d28670542f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n",
    "                                                                          toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                          tokenizer=tokenizer, \n",
    "                                                                          dataset=test_dataset, \n",
    "                                                                          num_samples=100)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51567807-c517-4702-886f-468babfa15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n",
    "                                                                        toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        dataset=test_dataset, \n",
    "                                                                        num_samples=100)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95b279-fb93-4610-9926-34ef7d3f2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
