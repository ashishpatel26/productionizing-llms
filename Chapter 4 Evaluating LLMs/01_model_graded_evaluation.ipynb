{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76c24a-a832-4828-8194-e9c9d4bb78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f813e-5458-4eae-93dd-6139017a3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "\n",
    "human_template  = \"{question}\"\n",
    "\n",
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\"\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deaceb7-a383-4dc1-bdce-451c3a344412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7add4-e759-4419-b7bd-1bc5e4092cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt | llm | output_parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61ea77-109a-4401-807e-061f2effbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "    ])\n",
    "    return chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321d216-9580-4eb7-9494-86b1917538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile test_assistant.py\n",
    "\n",
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "    \n",
    "    assistant = assistant_chain(\n",
    "      system_message,\n",
    "      human_template,\n",
    "      llm,\n",
    "      output_parser\n",
    "    )\n",
    "\n",
    "\n",
    "    answer = assistant.invoke({\"question\": question})\n",
    "\n",
    "    print(answer)\n",
    "\n",
    "    assert any(word in answer.lower() \\\n",
    "             for word in expected_words), \\\n",
    "    f\"Expected the assistant questions to include \\\n",
    "    '{expected_words}', but it did not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23854f-ed74-4c47-891e-90653b4c6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "\n",
    "eval_expected_words(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    expected_words\n",
    ")\n",
    "\n",
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\", \n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser()):\n",
    "    \n",
    "    assistant = assistant_chain(human_template, \n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "\n",
    "    answer = assistant.invoke({\"question\": question})\n",
    "    print(answer)\n",
    "\n",
    "    assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\"\n",
    "\n",
    "question  = \"Generate a quiz about Rome.\"\n",
    "decline_response = \"I'm sorry\"\n",
    "\n",
    "evaluate_refusal(\n",
    "    prompt_template,\n",
    "    question,\n",
    "    decline_response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5725e7-06ed-4f48-a2bd-153bbf84872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Test cases\n",
    "\"\"\"\n",
    "\n",
    "def test_science_quiz():\n",
    "  \n",
    "    question  = \"Generate a quiz about science.\"\n",
    "    expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "    eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_geography_quiz():\n",
    "    question  = \"Generate a quiz about geography.\"\n",
    "    expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
    "    eval_expected_words(\n",
    "      system_message,\n",
    "      question,\n",
    "      expected_subjects)\n",
    "\n",
    "def test_refusal_rome():\n",
    "    question  = \"Help me create a quiz about Rome\"\n",
    "    decline_response = \"I'm sorry\"\n",
    "    evaluate_refusal(\n",
    "      system_message,\n",
    "      question,\n",
    "      decline_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
