{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bb846f-4962-4813-b68e-d983fee66315",
   "metadata": {},
   "source": [
    "Transformers tiiuae/falcon-7b-instruct inference.\n",
    "\n",
    "Can be run locally or on Google Colab.\n",
    "Model size: 13.7 GB\n",
    "GPU memory or RAM needed: >= 14GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e440e79-2e35-481b-ade3-a6a907c5ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch einops -U accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55c1af-ce0c-4b1a-8334-fa5781ce75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f3d89-5fcb-4b29-8854-9ce9d121c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcb254-e042-49ef-b7ca-018f7b44e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    sequences = pipeline(\n",
    "    input(\"Ask something, here\"),\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6ac61-c976-4f7b-9ac1-4e6269a4d718",
   "metadata": {},
   "source": [
    "For bounded response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb71bab-26aa-4c37-8723-91299b82fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(q:str) -> None:\n",
    "    template = (\n",
    "        \"You are an artificial intelligence assistant.\" \n",
    "        \"The assistant gives helpful, detailed, and \"\n",
    "        \"polite answers to the user's questions. \"\n",
    "    )\n",
    "    query = template + q\n",
    "    sequences = pipeline(\n",
    "        query,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c3233-9bb7-4c41-b8b4-c6704aac36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = query(\"What is the meaning of life?\")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c17867-5645-4392-b136-b9a0fb955566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python OR Kernel",
   "language": "python",
   "name": "or"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
